// Updated AI Music Service with Real APIs
import { ElevenLabsService } from "./elevenlabs-voice-service"
import { MubertService } from "./mubert-music-service"
import { SunoService } from "./suno-music-service"

export interface MusicGenerationRequest {
  lyrics: string
  genre: string
  style: string
  tempo: number
  duration: string
  voiceId?: string
  title: string
  userId: string
  provider?: "suno" | "mubert"
}

export interface MusicGenerationResult {
  audioBuffer: Buffer
  metadata: {
    duration: number
    sampleRate: number
    bitRate: string
    format: string
    provider: string
    trackId?: string
  }
  stems?: {
    vocals: Buffer
    drums: Buffer
    bass: Buffer
    melody: Buffer
  }
}

export class AIMusicServiceV2 {
  // Main music generation function with real AI
  static async generateMusic(request: MusicGenerationRequest): Promise<MusicGenerationResult> {
    try {
      const provider = request.provider || "suno" // Default to Suno

      let audioBuffer: Buffer
      let metadata: any

      if (provider === "suno") {
        // Use Suno AI for music generation
        const sunoTracks = await SunoService.generateCustomMusic(
          request.lyrics,
          `${request.genre}, ${request.style}, ${request.tempo}bpm`,
          request.title,
          false, // Not instrumental since we have lyrics
        )

        if (sunoTracks.length === 0) {
          throw new Error("No tracks generated by Suno")
        }

        // Wait for completion
        const completedTracks = await SunoService.waitForCompletion([sunoTracks[0].id])

        if (completedTracks.length === 0) {
          throw new Error("Track generation failed or timed out")
        }

        const track = completedTracks[0]
        audioBuffer = await SunoService.downloadAudio(track.audio_url)

        metadata = {
          duration: track.duration,
          sampleRate: 44100,
          bitRate: "320kbps",
          format: "mp3",
          provider: "suno",
          trackId: track.id,
        }

        // If voice cloning is requested, replace vocals
        if (request.voiceId) {
          audioBuffer = await this.replaceVocalsWithClonedVoice(audioBuffer, request.lyrics, request.voiceId)
        }
      } else if (provider === "mubert") {
        // Use Mubert for instrumental generation
        const prompt = `${request.genre} ${request.style} music, ${request.tempo}bpm, ${request.lyrics}`
        const durationSeconds = this.parseDuration(request.duration)

        const track = await MubertService.generateWithPrompt(prompt, durationSeconds)
        audioBuffer = await MubertService.downloadTrack(track.download_link)

        metadata = {
          duration: track.duration,
          sampleRate: 44100,
          bitRate: "320kbps",
          format: "wav",
          provider: "mubert",
          trackId: track.track_id,
        }

        // Add vocals if lyrics provided
        if (request.lyrics && request.voiceId) {
          audioBuffer = await this.addVocalsToInstrumental(audioBuffer, request.lyrics, request.voiceId)
        }
      } else {
        throw new Error(`Unsupported provider: ${provider}`)
      }

      // Generate stems if needed (mock for now, can be enhanced)
      const stems = await this.generateStems(audioBuffer)

      return {
        audioBuffer,
        metadata,
        stems,
      }
    } catch (error) {
      console.error("AI music generation failed:", error)
      throw new Error(`Music generation failed: ${error}`)
    }
  }

  // Voice cloning with ElevenLabs
  static async cloneVoice(audioBuffer: Buffer, voiceName: string): Promise<string> {
    try {
      // Convert buffer to File object
      const audioFile = new File([audioBuffer], "voice_sample.wav", { type: "audio/wav" })

      const voiceId = await ElevenLabsService.cloneVoice({
        name: voiceName,
        description: `Cloned voice for ${voiceName}`,
        files: [audioFile],
        labels: {
          accent: "neutral",
          age: "adult",
          gender: "neutral",
          use_case: "music",
        },
      })

      return voiceId
    } catch (error) {
      console.error("Voice cloning failed:", error)
      throw new Error(`Voice cloning failed: ${error}`)
    }
  }

  // Text-to-speech with cloned voice
  static async synthesizeVoice(text: string, voiceId: string): Promise<Buffer> {
    try {
      return await ElevenLabsService.textToSpeech(text, voiceId, {
        model_id: "eleven_multilingual_v2",
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.75,
          style: 0.2,
          use_speaker_boost: true,
        },
      })
    } catch (error) {
      console.error("Voice synthesis failed:", error)
      throw new Error(`Voice synthesis failed: ${error}`)
    }
  }

  // Replace vocals in existing track with cloned voice
  private static async replaceVocalsWithClonedVoice(
    instrumentalBuffer: Buffer,
    lyrics: string,
    voiceId: string,
  ): Promise<Buffer> {
    try {
      // Generate vocals with cloned voice
      const vocalBuffer = await this.synthesizeVoice(lyrics, voiceId)

      // Mix vocals with instrumental (simplified - in production use audio processing library)
      // For now, return the vocal buffer (you'd want to properly mix these)
      return vocalBuffer
    } catch (error) {
      console.error("Vocal replacement failed:", error)
      return instrumentalBuffer // Return original if vocal replacement fails
    }
  }

  // Add vocals to instrumental track
  private static async addVocalsToInstrumental(
    instrumentalBuffer: Buffer,
    lyrics: string,
    voiceId: string,
  ): Promise<Buffer> {
    try {
      const vocalBuffer = await this.synthesizeVoice(lyrics, voiceId)

      // Mix vocals with instrumental (simplified)
      // In production, use proper audio mixing libraries like Web Audio API or FFmpeg
      return vocalBuffer // Placeholder - implement proper mixing
    } catch (error) {
      console.error("Vocal addition failed:", error)
      return instrumentalBuffer
    }
  }

  // Generate stems (mock implementation - enhance with real audio separation)
  private static async generateStems(audioBuffer: Buffer) {
    // In production, use AI-powered source separation like Spleeter or LALAL.AI
    const stemSize = Math.floor(audioBuffer.length / 4)

    return {
      vocals: audioBuffer.subarray(0, stemSize),
      drums: audioBuffer.subarray(stemSize, stemSize * 2),
      bass: audioBuffer.subarray(stemSize * 2, stemSize * 3),
      melody: audioBuffer.subarray(stemSize * 3),
    }
  }

  // Helper function to parse duration
  private static parseDuration(duration: string): number {
    const [minutes, seconds] = duration.split(":").map(Number)
    return minutes * 60 + (seconds || 0)
  }

  // Get available voices from ElevenLabs
  static async getAvailableVoices() {
    try {
      return await ElevenLabsService.getVoices()
    } catch (error) {
      console.error("Failed to get voices:", error)
      return []
    }
  }

  // Check service status
  static async checkServiceStatus() {
    const status = {
      elevenlabs: false,
      suno: false,
      mubert: false,
    }

    try {
      await ElevenLabsService.getVoices()
      status.elevenlabs = true
    } catch (error) {
      console.error("ElevenLabs service unavailable:", error)
    }

    try {
      await SunoService.getAccountInfo()
      status.suno = true
    } catch (error) {
      console.error("Suno service unavailable:", error)
    }

    try {
      // Test Mubert with a simple request
      status.mubert = true // Assume available for now
    } catch (error) {
      console.error("Mubert service unavailable:", error)
    }

    return status
  }
}
